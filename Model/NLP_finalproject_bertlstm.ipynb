{"cells":[{"cell_type":"markdown","metadata":{"id":"02Snqj4th5o5"},"source":["**Activate google colab**\n"]},{"cell_type":"code","execution_count":207,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":739,"status":"ok","timestamp":1610253619819,"user":{"displayName":"陳鈞澤","photoUrl":"","userId":"02348403863745472562"},"user_tz":-480},"id":"suPGVKGqhmX_","outputId":"c405a0f6-e54f-4396-ab3b-173bd93a8dc2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":208,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4093,"status":"ok","timestamp":1610253623184,"user":{"displayName":"陳鈞澤","photoUrl":"","userId":"02348403863745472562"},"user_tz":-480},"id":"h8xiWbg8rVPH","outputId":"117497e8-0650-4bd6-ed79-f59638a5cf2d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: ckip-transformers in /usr/local/lib/python3.6/dist-packages (0.2.1)\n","Requirement already satisfied: transformers\u003e=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ckip-transformers) (4.1.1)\n","Requirement already satisfied: torch\u003e=1.1.0 in /usr/local/lib/python3.6/dist-packages (from ckip-transformers) (1.7.0+cu101)\n","Requirement already satisfied: tqdm\u003e=4.27 in /usr/local/lib/python3.6/dist-packages (from ckip-transformers) (4.41.1)\n","Requirement already satisfied: dataclasses; python_version \u003c \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers\u003e=3.5.0-\u003eckip-transformers) (0.8)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers\u003e=3.5.0-\u003eckip-transformers) (1.19.4)\n","Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers\u003e=3.5.0-\u003eckip-transformers) (0.9.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers\u003e=3.5.0-\u003eckip-transformers) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers\u003e=3.5.0-\u003eckip-transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers\u003e=3.5.0-\u003eckip-transformers) (3.0.12)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers\u003e=3.5.0-\u003eckip-transformers) (0.0.43)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers\u003e=3.5.0-\u003eckip-transformers) (20.8)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch\u003e=1.1.0-\u003eckip-transformers) (3.7.4.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch\u003e=1.1.0-\u003eckip-transformers) (0.16.0)\n","Requirement already satisfied: chardet\u003c4,\u003e=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests-\u003etransformers\u003e=3.5.0-\u003eckip-transformers) (3.0.4)\n","Requirement already satisfied: idna\u003c3,\u003e=2.5 in /usr/local/lib/python3.6/dist-packages (from requests-\u003etransformers\u003e=3.5.0-\u003eckip-transformers) (2.10)\n","Requirement already satisfied: certifi\u003e=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests-\u003etransformers\u003e=3.5.0-\u003eckip-transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,\u003c1.26,\u003e=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests-\u003etransformers\u003e=3.5.0-\u003eckip-transformers) (1.24.3)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses-\u003etransformers\u003e=3.5.0-\u003eckip-transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses-\u003etransformers\u003e=3.5.0-\u003eckip-transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses-\u003etransformers\u003e=3.5.0-\u003eckip-transformers) (1.0.0)\n","Requirement already satisfied: pyparsing\u003e=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging-\u003etransformers\u003e=3.5.0-\u003eckip-transformers) (2.4.7)\n"]}],"source":["pip install ckip-transformers"]},{"cell_type":"markdown","metadata":{"id":"m-ccNlBliBhO"},"source":["**import module**"]},{"cell_type":"code","execution_count":209,"metadata":{"executionInfo":{"elapsed":4087,"status":"ok","timestamp":1610253623186,"user":{"displayName":"陳鈞澤","photoUrl":"","userId":"02348403863745472562"},"user_tz":-480},"id":"DQx3hhqViBEN"},"outputs":[],"source":["# old  module\n","import numpy as np\n","import tensorflow as tf  \n","from tensorflow import keras\n","from tensorflow.keras import layers \n","\n","#new  module\n","import torch\n","from torch.utils.data import DataLoader, TensorDataset\n","from torch.nn import Transformer\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import numpy as np\n","import os\n","import torch.optim as optim\n","import random\n","\n","random.seed(42)"]},{"cell_type":"markdown","metadata":{"id":"ixkI3Q-xjZFq"},"source":["**read Data**"]},{"cell_type":"code","execution_count":210,"metadata":{"executionInfo":{"elapsed":4082,"status":"ok","timestamp":1610253623187,"user":{"displayName":"陳鈞澤","photoUrl":"","userId":"02348403863745472562"},"user_tz":-480},"id":"Qc1fWCHRjj9b"},"outputs":[],"source":["def read_data(file):\n","    #load and clean'\\n' \n","    with open(file,'r') as ff:\n","        test = ff.readlines()\n","    test = [i.replace('\\n','') for i in test]\n","    test = [i.split(' ') for i in test]\n","    \n","    #reshape data\n","    text_final = []\n","    BIO_final = [] \n","    text_tmp = []\n","    BIO_tmp = []\n","    for i in test:\n","        if len(i) \u003e 1:\n","            text_tmp.extend([i[0]])\n","            BIO_tmp.append(i[1])\n","        elif len(i) == 1:\n","            text_final.append(text_tmp)\n","            BIO_final.append(BIO_tmp)\n","            text_tmp = []\n","            BIO_tmp = []       \n","    cls_value = [i.pop(0) for i in BIO_final]\n","    [i.pop(0) for i in text_final]\n","    return text_final, BIO_final, cls_value\n","\n","def label2index(label,label_dict):\n","    index_label = []\n","    for i in label:\n","        lab_tmp = [label_dict[j]for j in i] \n","        index_label.append(lab_tmp)\n","    return index_label"]},{"cell_type":"markdown","metadata":{"id":"9DdXD6cbnwag"},"source":["**get representation**"]},{"cell_type":"code","execution_count":211,"metadata":{"executionInfo":{"elapsed":4077,"status":"ok","timestamp":1610253623187,"user":{"displayName":"陳鈞澤","photoUrl":"","userId":"02348403863745472562"},"user_tz":-480},"id":"in0UEv_envYH"},"outputs":[],"source":["def get_representation(output):\n","    # shape: (seq_len, vocab_size)\n","    hidden_states = output[1]\n","\n","    token_embeddings = torch.stack(hidden_states, dim=0)\n","    # remove dimension 1 (batches)\n","    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n","    # swap dimension 0 and 1\n","    token_embeddings = token_embeddings.permute(1, 0, 2)\n","    # the last hidden layer output (2+seq_len, 768)\n","    hidden_states = [token[-1] for token in token_embeddings]\n","\n","    return hidden_states"]},{"cell_type":"markdown","metadata":{"id":"dpf0I1k4oc9g"},"source":["**decoder for ID**"]},{"cell_type":"code","execution_count":212,"metadata":{"executionInfo":{"elapsed":4071,"status":"ok","timestamp":1610253623188,"user":{"displayName":"陳鈞澤","photoUrl":"","userId":"02348403863745472562"},"user_tz":-480},"id":"aKRBq7a8oOqM"},"outputs":[],"source":["# decoder for ID\n","class ID_module(nn.Module):\n","    def __init__(self):\n","        super(ID_module, self).__init__()\n","        self.linear = nn.Linear(768, 2)\n","    \n","    def forward(self, X):\n","        X = self.linear(X)\n","        return X"]},{"cell_type":"markdown","metadata":{"id":"5OUNLpf4oiqX"},"source":["**decoder for SF**"]},{"cell_type":"code","execution_count":213,"metadata":{"executionInfo":{"elapsed":4067,"status":"ok","timestamp":1610253623189,"user":{"displayName":"陳鈞澤","photoUrl":"","userId":"02348403863745472562"},"user_tz":-480},"id":"7cy0MbTHoQuw"},"outputs":[],"source":["# decoder for SF\n","class SF_module(nn.Module):\n","    def __init__(self):\n","        super(SF_module, self).__init__()\n","        self.linear = nn.Linear(768, n_classes)\n","    \n","    def forward(self, X):\n","        X = self.linear(X)\n","        return X"]},{"cell_type":"markdown","metadata":{"id":"lsYOq34gops6"},"source":["**Train ID**"]},{"cell_type":"code","execution_count":214,"metadata":{"executionInfo":{"elapsed":4062,"status":"ok","timestamp":1610253623190,"user":{"displayName":"陳鈞澤","photoUrl":"","userId":"02348403863745472562"},"user_tz":-480},"id":"nUzuHrYfotjb"},"outputs":[],"source":["# dataset: [[sen_1, 1], [sen_2, 1], [sen_3, 2], ...]\n","\n","def train_ID(ID_model, dataset, epochs, lr):\n","    ID_model.train()\n","    \n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(ID_model.parameters(), lr=lr, momentum=0.9)\n","\n","    for epoch in range(epochs):\n","        totalLoss = 0\n","        accuracy = 0\n","        count = 0\n","\n","        for X, y in dataset:\n","            y = torch.LongTensor([int(y)]).to(DEVICE)\n","            optimizer.zero_grad()\n","\n","            X = \" \".join(X)\n","            X_encoding = tokenizer.encode_plus(X, add_special_tokens=True, return_tensors='pt')\n","            X_ids = X_encoding['input_ids'].to(DEVICE)\n","\n","            with torch.no_grad():\n","                output = model(X_ids)\n","                \n","            # get the 768-d representation of [CLS]\n","            ID_input = get_representation(output)[0]\n","            ID_output = ID_model(ID_input).unsqueeze(0)\n","\n","            ##### record the loss? #####\n","            _, predicted = torch.max(ID_output.data, 1)\n","            count += len(X_ids) - 2\n","            accuracy += (predicted == y).sum().item()\n","\n","            loss = criterion(ID_output, y)\n","\n","            totalLoss += loss.item()\n","            # print(\"Loss: {}\".format(totalLoss))\n","            \n","            loss.backward()\n","            optimizer.step()\n","\n","        print(\"Train Loss: {}\".format(totalLoss / count))\n","        print(\"Train Accuracy: {}\".format(accuracy / count))\n","\n","    return ID_model"]},{"cell_type":"markdown","metadata":{"id":"Jtfpn8w0o5KV"},"source":["**Train SF**"]},{"cell_type":"code","execution_count":215,"metadata":{"executionInfo":{"elapsed":4056,"status":"ok","timestamp":1610253623190,"user":{"displayName":"陳鈞澤","photoUrl":"","userId":"02348403863745472562"},"user_tz":-480},"id":"TuVAXqbVo6_W"},"outputs":[],"source":["# dataset: [[sen_1, [1,1,1,2,3,...]], [sen_2, [1,1,2,3,...]], [sen_3, [1,1,1,2,...]], ...]\n","\n","def train_SF(SF_model, dataset, epochs, lr):\n","    SF_model.train()\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.SGD(SF_model.parameters(), lr=lr, momentum=0.9)\n","\n","    for epoch in range(epochs):\n","        totalLoss = 0\n","        count = 0\n","        y_true = []\n","        y_pred = []\n","\n","        for X, y in dataset:\n","            y_true.extend(y)\n","            optimizer.zero_grad()\n","\n","            X = \" \".join(X)\n","            X_encoding = tokenizer.encode_plus(X, add_special_tokens=True, return_tensors='pt')\n","            X_ids = X_encoding['input_ids'].to(DEVICE)\n","\n","            with torch.no_grad():\n","                output = model(X_ids)\n","\n","            # get the 768-d representation of other tokens than [CLS]\n","            SF_input = get_representation(output)[1:-1]\n","            # for each position, predict a label with SF_model and back propagate the loss\n","            for p in range(len(X_ids[0]) - 2):\n","                y_single = torch.LongTensor([y[p]]).to(DEVICE)\n","                SF_output = SF_model(SF_input[p]).unsqueeze(0)\n","\n","                # print(y_single)\n","\n","                y_pred.append(torch.argmax(SF_output).item())\n","\n","                # record the loss\n","                loss = criterion(SF_output, y_single)\n","                # print(\"Loss: {:2f}\".format(loss))\n","                count += 1\n","                totalLoss += loss.item()*5\n","    \n","                loss.backward()\n","                optimizer.step()\n","\n","        print(\"Train Loss: {:2f}\".format(totalLoss / count))\n","        # print(\"Train Loss: {}\".format(totalLoss))\n","        # print f1-score\n","        print( \"F1 score: {:2f}\".format(f1_score(y_true, y_pred, average = 'macro')) )\n","\n","    return SF_model"]},{"cell_type":"markdown","metadata":{"id":"eumpw1_fjNBq"},"source":["**load ckip BERT**"]},{"cell_type":"code","execution_count":216,"metadata":{"executionInfo":{"elapsed":9741,"status":"ok","timestamp":1610253628880,"user":{"displayName":"陳鈞澤","photoUrl":"","userId":"02348403863745472562"},"user_tz":-480},"id":"HJqRjIgPjPwI"},"outputs":[],"source":["from transformers import (\n","   BertTokenizerFast,\n","   AutoModelForMaskedLM,\n","   AutoModelForTokenClassification,\n",")\n","\n","DEVICE = torch.device('cpu')\n","\n","# language model\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\n","model = AutoModelForMaskedLM.from_pretrained('ckiplab/bert-base-chinese', output_hidden_states=True).to(DEVICE)"]},{"cell_type":"markdown","metadata":{"id":"XarVOyk9oEOk"},"source":["**id_label_translation**"]},{"cell_type":"code","execution_count":217,"metadata":{"executionInfo":{"elapsed":9735,"status":"ok","timestamp":1610253628882,"user":{"displayName":"陳鈞澤","photoUrl":"","userId":"02348403863745472562"},"user_tz":-480},"id":"JYocSX0yoJys"},"outputs":[],"source":["n_classes = 5\n","intent_to_id = {\n","    \"income\": 0,\n","    \"expense\": 1\n","}\n","slot_to_id = {\n","    \"O\": 1,\n","    \"B-item\": 2,\n","    \"I-item\": 3,\n","    \"B-money\": 4,\n","    \"I-money\": 5\n","}\n","\n","id_to_intent = {v: k for (k, v) in intent_to_id.items()}\n","id_to_slot = {v: k for (k, v) in slot_to_id.items()}\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9VYp8zE9qpup"},"source":["**load train data**"]},{"cell_type":"code","execution_count":218,"metadata":{"executionInfo":{"elapsed":32126,"status":"ok","timestamp":1610253651278,"user":{"displayName":"陳鈞澤","photoUrl":"","userId":"02348403863745472562"},"user_tz":-480},"id":"uxeZJlumqt1a"},"outputs":[],"source":["data_file = \"/content/gdrive/MyDrive/NLP/final_project/final_dataset/train_data_income99_expense_99.txt\"\n","text_final, BIO_final, cls_value = read_data(data_file)\n","\n","##shuffle data set\n","random.seed(42)\n","ID_dataset = [z for z in zip(text_final, cls_value)]\n","random.shuffle(ID_dataset)\n","random.seed(42)\n","\n","BIO_final_id = label2index(BIO_final, slot_to_id)\n","SF_dataset = [z for z in zip(text_final, BIO_final_id)]\n","random.shuffle(SF_dataset)\n","\n","## get bert representation\n","x_bert = []\n","y_true = []\n","for X, y in SF_dataset:\n","  y_true.append(y)\n","\n","  X = \" \".join(X)\n","  X_encoding = tokenizer.encode_plus(X, add_special_tokens=True, return_tensors='pt')\n","  X_ids = X_encoding['input_ids'].to(DEVICE)\n","  with torch.no_grad():\n","      output = model(X_ids)\n","  # get the 768-d representation of other tokens than [CLS]\n","  SF_input = get_representation(output)[1:-1]\n","  SF_input = [np.array(i) for i in SF_input]\n","  x_bert.append(SF_input)\n","\n","## pading for X and Y\n","for i in x_bert:\n","  while len(i) \u003c 18:\n","    i.append(np.zeros(768))\n","for i in y_true:\n","  while len(i) \u003c 18:\n","    i.append(0)"]},{"cell_type":"markdown","metadata":{"id":"jIPs6g39D1nQ"},"source":["**load valid data**"]},{"cell_type":"code","execution_count":219,"metadata":{"executionInfo":{"elapsed":40521,"status":"ok","timestamp":1610253659679,"user":{"displayName":"陳鈞澤","photoUrl":"","userId":"02348403863745472562"},"user_tz":-480},"id":"6pJi0Qhor_CK"},"outputs":[],"source":["data_file = \"/content/gdrive/MyDrive/NLP/final_project/final_dataset/valid_data_income31_expense40.txt\"\n","text_final, BIO_final, cls_value = read_data(data_file)\n","\n","##shuffle data set\n","random.seed(42)\n","ID_dataset = [z for z in zip(text_final, cls_value)]\n","random.shuffle(ID_dataset)\n","random.seed(42)\n","\n","BIO_final_id = label2index(BIO_final, slot_to_id)\n","SF_dataset = [z for z in zip(text_final, BIO_final_id)]\n","random.shuffle(SF_dataset)\n","\n","## get bert representation\n","x_bert_valid = []\n","y_true_valid = []\n","for X, y in SF_dataset:\n","  y_true_valid.append(y)\n","\n","  X = \" \".join(X)\n","  X_encoding = tokenizer.encode_plus(X, add_special_tokens=True, return_tensors='pt')\n","  X_ids = X_encoding['input_ids'].to(DEVICE)\n","  with torch.no_grad():\n","      output = model(X_ids)\n","  # get the 768-d representation of other tokens than [CLS]\n","  SF_input = get_representation(output)[1:-1]\n","  SF_input = [np.array(i) for i in SF_input]\n","  x_bert_valid.append(SF_input)\n","\n","## pading for X and Y\n","for i in x_bert_valid:\n","  while len(i) \u003c 20:\n","    i.append(np.zeros(768))\n","for i in y_true_valid:\n","  while len(i) \u003c 20:\n","    i.append(0)"]},{"cell_type":"code","execution_count":220,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40516,"status":"ok","timestamp":1610253659681,"user":{"displayName":"陳鈞澤","photoUrl":"","userId":"02348403863745472562"},"user_tz":-480},"id":"fR4E26TgGUJd","outputId":"caf770d2-e42d-496a-a412-0722a9fa1925"},"outputs":[{"name":"stdout","output_type":"stream","text":["198\n","198\n","71\n","71\n"]}],"source":["##change format\n","x_bert = [np.array(i) for i in x_bert]\n","y_true = [np.array(i) for i in y_true]\n","x_bert_valid = [np.array(i) for i in x_bert_valid]\n","y_true_valid = [np.array(i) for i in y_true_valid]\n","\n","x_bert = np.array(x_bert)\n","y_true = np.array(y_true)\n","x_bert_valid = np.array(x_bert_valid)\n","y_true_valid = np.array(y_true_valid)\n","\n","\n","print(len(x_bert))\n","print(len(y_true))\n","print(len(x_bert_valid))\n","print(len(y_true_valid))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5qaagDfU7eXp"},"source":["**bidirectional LSTM(max sequence length 18)**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"c0YZwDA5vYRP"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n","WARNING:tensorflow:Model was constructed with shape (None, 18, 768) for input KerasTensor(type_spec=TensorSpec(shape=(None, 18, 768), dtype=tf.float32, name='masking_22_input'), name='masking_22_input', description=\"created by layer 'masking_22_input'\"), but it was called on an input with incompatible shape (1, 20, 768).\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:tensorflow:Model was constructed with shape (None, 18, 768) for input KerasTensor(type_spec=TensorSpec(shape=(None, 18, 768), dtype=tf.float32, name='masking_22_input'), name='masking_22_input', description=\"created by layer 'masking_22_input'\"), but it was called on an input with incompatible shape (1, 20, 768).\n"]},{"name":"stdout","output_type":"stream","text":["198/198 - 6s - loss: 0.2043 - accuracy: 0.9041 - val_loss: 0.0993 - val_accuracy: 0.9553\n","\n","Epoch 00001: saving model to /content/gdrive/MyDrive/NLP/final_project/train_record/weights-improvement-01.hdf5\n","Epoch 2/10\n","198/198 - 2s - loss: 0.0558 - accuracy: 0.9788 - val_loss: 0.0830 - val_accuracy: 0.9591\n","\n","Epoch 00002: saving model to /content/gdrive/MyDrive/NLP/final_project/train_record/weights-improvement-02.hdf5\n","Epoch 3/10\n","198/198 - 2s - loss: 0.0285 - accuracy: 0.9894 - val_loss: 0.0909 - val_accuracy: 0.9617\n","\n","Epoch 00003: saving model to /content/gdrive/MyDrive/NLP/final_project/train_record/weights-improvement-03.hdf5\n","Epoch 4/10\n","198/198 - 2s - loss: 0.0133 - accuracy: 0.9940 - val_loss: 0.1014 - val_accuracy: 0.9617\n","\n","Epoch 00004: saving model to /content/gdrive/MyDrive/NLP/final_project/train_record/weights-improvement-04.hdf5\n","Epoch 5/10\n","198/198 - 2s - loss: 0.0066 - accuracy: 0.9968 - val_loss: 0.1092 - val_accuracy: 0.9591\n","\n","Epoch 00005: saving model to /content/gdrive/MyDrive/NLP/final_project/train_record/weights-improvement-05.hdf5\n","Epoch 6/10\n","198/198 - 2s - loss: 0.0045 - accuracy: 0.9991 - val_loss: 0.1156 - val_accuracy: 0.9617\n","\n","Epoch 00006: saving model to /content/gdrive/MyDrive/NLP/final_project/train_record/weights-improvement-06.hdf5\n","Epoch 7/10\n","198/198 - 2s - loss: 0.0026 - accuracy: 0.9995 - val_loss: 0.1277 - val_accuracy: 0.9642\n","\n","Epoch 00007: saving model to /content/gdrive/MyDrive/NLP/final_project/train_record/weights-improvement-07.hdf5\n","Epoch 8/10\n","198/198 - 2s - loss: 0.0024 - accuracy: 0.9991 - val_loss: 0.1438 - val_accuracy: 0.9630\n","\n","Epoch 00008: saving model to /content/gdrive/MyDrive/NLP/final_project/train_record/weights-improvement-08.hdf5\n","Epoch 9/10\n","198/198 - 2s - loss: 0.0012 - accuracy: 0.9991 - val_loss: 0.1444 - val_accuracy: 0.9617\n","\n","Epoch 00009: saving model to /content/gdrive/MyDrive/NLP/final_project/train_record/weights-improvement-09.hdf5\n","Epoch 10/10\n","198/198 - 2s - loss: 0.0015 - accuracy: 0.9995 - val_loss: 0.1631 - val_accuracy: 0.9604\n","\n","Epoch 00010: saving model to /content/gdrive/MyDrive/NLP/final_project/train_record/weights-improvement-10.hdf5\n","Model: \"sequential_22\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","masking_22 (Masking)         (None, 18, 768)           0         \n","_________________________________________________________________\n","lstm_37 (LSTM)               (None, 18, 32)            102528    \n","_________________________________________________________________\n","dropout_13 (Dropout)         (None, 18, 32)            0         \n","_________________________________________________________________\n","dense_22 (Dense)             (None, 18, 6)             198       \n","=================================================================\n","Total params: 102,726\n","Trainable params: 102,726\n","Non-trainable params: 0\n","_________________________________________________________________\n"]},{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as lstm_cell_52_layer_call_and_return_conditional_losses, lstm_cell_52_layer_call_fn, lstm_cell_52_layer_call_fn, lstm_cell_52_layer_call_and_return_conditional_losses, lstm_cell_52_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n","WARNING:absl:Found untraced functions such as lstm_cell_52_layer_call_and_return_conditional_losses, lstm_cell_52_layer_call_fn, lstm_cell_52_layer_call_fn, lstm_cell_52_layer_call_and_return_conditional_losses, lstm_cell_52_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/NLP/final_project/assets\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Assets written to: /content/gdrive/MyDrive/NLP/final_project/assets\n"]}],"source":["#set parameter\n","samples, timesteps, features = len(x_bert), len(x_bert[0]), len(x_bert[0][0])\n","\n","network = keras.Sequential()\n","\n","#masking layer\n","network.add(keras.layers.Masking(mask_value=0.,\n","                                  input_shape=(timesteps, features)))\n","#model.LSTM\n","forward_layer = layers.LSTM(32, return_sequences=True)\n","# backward_layer = layers.LSTM(32, return_sequences=True, go_backwards=True)\n","# network.add(layers.Bidirectional(forward_layer, backward_layer=backward_layer))\n","network.add(forward_layer)\n","network.add(layers.Dropout(0.1))\n","# network.add(layers.LSTM(64,return_sequences = 'True'))\n","\n","#output layer\n","network.add(layers.Dense(6,activation='softmax'))\n","\n","#error\n","network.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'RMSprop',metrics = ['accuracy'])\n","\n","# checkpoint\n","filepath=\"/content/gdrive/MyDrive/NLP/final_project/train_record/weights-improvement-{epoch:02d}.hdf5\"\n","checkpoint = keras.callbacks.ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only  = False,save_weights_only = False,save_freq=\"epoch\")\n","callbacks_list = [checkpoint]\n","#\n","train_history = network.fit(x = x_bert,y = y_true,validation_data=(x_bert_valid, y_true_valid), epochs = 10, batch_size = 1, callbacks = callbacks_list, verbose = 2)\n","network.summary()\n","network.save('/content/gdrive/MyDrive/NLP/final_project')"]},{"cell_type":"markdown","metadata":{"id":"GrffwNLjJ0GZ"},"source":["**PREDICT**"]},{"cell_type":"code","execution_count":201,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15048,"status":"ok","timestamp":1610253467637,"user":{"displayName":"陳鈞澤","photoUrl":"","userId":"02348403863745472562"},"user_tz":-480},"id":"YMuMC9BUwPep","outputId":"3ca862ea-9eb3-424e-de40-2e44aa28a49c"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) \u003e 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n","  warnings.warn('`model.predict_classes()` is deprecated and '\n"]},{"name":"stdout","output_type":"stream","text":["71\n"]}],"source":["data_file = \"/content/gdrive/MyDrive/NLP/final_project/final_dataset/valid_data_income31_expense40.txt\"\n","text_final, BIO_final, cls_value = read_data(data_file)\n","\n","##shuffle data set\n","random.seed(42)\n","ID_dataset = [z for z in zip(text_final, cls_value)]\n","random.shuffle(ID_dataset)\n","random.seed(42)\n","\n","BIO_final_id = label2index(BIO_final, slot_to_id)\n","SF_dataset = [z for z in zip(text_final, BIO_final_id)]\n","random.shuffle(SF_dataset)\n","\n","## get bert representation\n","x_bert_valid = []\n","y_true_valid = []\n","for X, y in SF_dataset:\n","  y_true_valid.append(y)\n","\n","  X = \" \".join(X)\n","  X_encoding = tokenizer.encode_plus(X, add_special_tokens=True, return_tensors='pt')\n","  X_ids = X_encoding['input_ids'].to(DEVICE)\n","  with torch.no_grad():\n","      output = model(X_ids)\n","  # get the 768-d representation of other tokens than [CLS]\n","  SF_input = get_representation(output)[1:-1]\n","  SF_input = [np.array(i) for i in SF_input]\n","  x_bert_valid.append(SF_input)\n","\n","## predict\n","y_pred = []\n","for i in x_bert_valid:\n","  tt = []\n","  tt.append(i)\n","  output_label = network.predict_classes(np.array(tt))\n","  y_pred.append(output_label)\n","\n","print(len(y_pred))\n","##handinput\n","# X = '設計一款遊戲賺了1043043元'\n","# X = \" \".join(X)\n","# X_encoding = tokenizer.encode_plus(X, add_special_tokens=True, return_tensors='pt')\n","# X_ids = X_encoding['input_ids'].to(DEVICE)\n","# with torch.no_grad():\n","#     output = model(X_ids)\n","# # get the 768-d representation of other tokens than [CLS]\n","# LSTM_input = get_representation(output)[1:-1]\n","# LSTM_input = [np.array(i) for i in LSTM_input]\n","# tt=[]\n","# tt.append(np.array(LSTM_input))\n","# output_label = network.predict_classes(np.array(tt))\n"]},{"cell_type":"code","execution_count":165,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":841,"status":"ok","timestamp":1610249854168,"user":{"displayName":"陳鈞澤","photoUrl":"","userId":"02348403863745472562"},"user_tz":-480},"id":"ItG-qio8F4Kd","outputId":"aab49928-3ba0-4c36-84bb-ca00211f0d81"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 13, 768)\n"]}],"source":["#caluculate F1 score\n","from sklearn.metrics import f1_score, recall_score, precision_score\n"]},{"cell_type":"code","execution_count":205,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":790,"status":"ok","timestamp":1610253552702,"user":{"displayName":"陳鈞澤","photoUrl":"","userId":"02348403863745472562"},"user_tz":-480},"id":"u3x2BlCOF_OO","outputId":"85b8de7b-ced7-4431-8276-be651940cc69"},"outputs":[{"name":"stdout","output_type":"stream","text":["[[1 1 1 1 1 1 4 5 5 5 5 5 5 5 5 5 5 5 5 5]]\n","['公', '司', '今', '年', '賺', '了', '1', '2', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '0', '元']\n"]}],"source":["print(y_pred[3])\n","print(SF_dataset[3][0])\n","print(y_true_valid)\n"]},{"cell_type":"code","execution_count":200,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":960},"executionInfo":{"elapsed":922,"status":"error","timestamp":1610253417560,"user":{"displayName":"陳鈞澤","photoUrl":"","userId":"02348403863745472562"},"user_tz":-480},"id":"rullN1c0iQ5X","outputId":"096c9a59-6441-4353-b356-2d4cc1fbfa6d"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Model was constructed with shape (None, 18, 768) for input KerasTensor(type_spec=TensorSpec(shape=(None, 18, 768), dtype=tf.float32, name='masking_20_input'), name='masking_20_input', description=\"created by layer 'masking_20_input'\"), but it was called on an input with incompatible shape (None, 768).\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) \u003e 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n","  warnings.warn('`model.predict_classes()` is deprecated and '\n","WARNING:tensorflow:Model was constructed with shape (None, 18, 768) for input KerasTensor(type_spec=TensorSpec(shape=(None, 18, 768), dtype=tf.float32, name='masking_20_input'), name='masking_20_input', description=\"created by layer 'masking_20_input'\"), but it was called on an input with incompatible shape (None, 768).\n"]},{"ename":"ValueError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m\u003cipython-input-200-ec28dff20931\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# tt=[]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# tt.append(np.array(LSTM_input))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 13\u001b[0;31m \u001b[0moutput_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36mpredict_classes\u001b[0;34m(self, x, batch_size, verbose)\u001b[0m\n\u001b[1;32m    457\u001b[0m                   \u001b[0;34m'  if your model does binary classification '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                   '  (e.g. if it uses a `sigmoid` last-layer activation).')\n\u001b[0;32m--\u003e 459\u001b[0;31m     \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m\u003e\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mproba\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1627\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 1629\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1631\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m-\u003e 2941\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3356\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3357\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-\u003e 3358\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3279\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-\u003e 3280\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3281\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-\u003e 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--\u003e 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1478 predict_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1468 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1461 run_step  **\n        outputs = model.predict_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:1434 predict_step\n        return self(x, training=False)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:1012 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:375 call\n        return super(Sequential, self).call(inputs, training=training, mask=mask)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:425 call\n        inputs, training=training, mask=mask)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/functional.py:560 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/wrappers.py:539 __call__\n        return super(Bidirectional, self).__call__(inputs, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:223 assert_input_compatibility\n        str(tuple(shape)))\n\n    ValueError: Input 0 of layer bidirectional_14 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (None, 768)\n"]}],"source":["#handinput\n","X = '設計一款遊戲賺了1043043元'\n","X = \" \".join(X)\n","X_encoding = tokenizer.encode_plus(X, add_special_tokens=True, return_tensors='pt')\n","X_ids = X_encoding['input_ids'].to(DEVICE)\n","with torch.no_grad():\n","    output = model(X_ids)\n","# get the 768-d representation of other tokens than [CLS]\n","LSTM_input = get_representation(output)[1:-1]\n","LSTM_input = [np.array(i) for i in LSTM_input]\n","# tt=[]\n","# tt.append(np.array(LSTM_input))\n","output_label = network.predict_classes(np.array(LSTM_input))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"85kyBsTalPSX"},"outputs":[],"source":[""]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPdExhS/OIpK/9NymET84ji","name":"NLP_finalproject_bertlstm.ipynb","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}