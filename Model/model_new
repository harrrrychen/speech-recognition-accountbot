{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model","provenance":[],"collapsed_sections":[],"mount_file_id":"1BuE1G1SpJjsKVzBEwBuQ4f0AYaDId-HZ","authorship_tag":"ABX9TyOuVOSEx72b5+Abl/jdCwVc"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4HXg6Ope_by4","executionInfo":{"status":"ok","timestamp":1610185956706,"user_tz":-480,"elapsed":9508,"user":{"displayName":"曹峻瑋","photoUrl":"","userId":"01275852677691706051"}},"outputId":"c9e94c7b-84e2-4e2f-f126-a36681f7a2d3"},"source":["pip install ckip-transformers"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting ckip-transformers\n","  Downloading https://files.pythonhosted.org/packages/19/53/81d1a8895cbbc02bf32771a7a43d78ad29a8c281f732816ac422bf54f937/ckip_transformers-0.2.1-py3-none-any.whl\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from ckip-transformers) (4.41.1)\n","Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from ckip-transformers) (1.7.0+cu101)\n","Collecting transformers>=3.5.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/50/0c/7d5950fcd80b029be0a8891727ba21e0cd27692c407c51261c3c921f6da3/transformers-4.1.1-py3-none-any.whl (1.5MB)\n","\u001b[K     |████████████████████████████████| 1.5MB 6.2MB/s \n","\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->ckip-transformers) (0.16.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->ckip-transformers) (3.7.4.3)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->ckip-transformers) (0.8)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.1.0->ckip-transformers) (1.19.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers>=3.5.0->ckip-transformers) (20.8)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers>=3.5.0->ckip-transformers) (2019.12.20)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 15.8MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers>=3.5.0->ckip-transformers) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers>=3.5.0->ckip-transformers) (3.0.12)\n","Collecting tokenizers==0.9.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/1c/e789a8b12e28be5bc1ce2156cf87cb522b379be9cadc7ad8091a4cc107c4/tokenizers-0.9.4-cp36-cp36m-manylinux2010_x86_64.whl (2.9MB)\n","\u001b[K     |████████████████████████████████| 2.9MB 18.5MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers>=3.5.0->ckip-transformers) (2.4.7)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.5.0->ckip-transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.5.0->ckip-transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers>=3.5.0->ckip-transformers) (1.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.5.0->ckip-transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.5.0->ckip-transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.5.0->ckip-transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers>=3.5.0->ckip-transformers) (2.10)\n","Building wheels for collected packages: sacremoses\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893261 sha256=dbce81cb5362c0154367e4a023caa4fcc7ee7da06c5549452879a97e1107e7f9\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built sacremoses\n","Installing collected packages: sacremoses, tokenizers, transformers, ckip-transformers\n","Successfully installed ckip-transformers-0.2.1 sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.1.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CsW8NqAQ-onY","executionInfo":{"status":"ok","timestamp":1610185960259,"user_tz":-480,"elapsed":12358,"user":{"displayName":"曹峻瑋","photoUrl":"","userId":"01275852677691706051"}}},"source":["import torch\r\n","from torch.utils.data import DataLoader, TensorDataset\r\n","from torch.nn import Transformer\r\n","import torch.nn as nn\r\n","import torch.nn.functional as F\r\n","import numpy as np\r\n","import os\r\n","import torch.optim as optim\r\n","from sklearn.metrics import f1_score\r\n","\r\n","import random\r\n","random.seed(42)"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"cPkgkVMs_uL_"},"source":["from transformers import (\r\n","   BertTokenizerFast,\r\n","   AutoModelForMaskedLM,\r\n","   AutoModelForTokenClassification,\r\n",")\r\n","\r\n","DEVICE = torch.device('cpu')\r\n","\r\n","# language model\r\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-chinese')\r\n","model = AutoModelForMaskedLM.from_pretrained('ckiplab/bert-base-chinese', output_hidden_states=True).to(DEVICE)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DWPzsDgpIj3P","executionInfo":{"status":"ok","timestamp":1610162068656,"user_tz":-480,"elapsed":1025,"user":{"displayName":"曹峻瑋","photoUrl":"","userId":"01275852677691706051"}}},"source":["def read_data(file):\r\n","    #load and clean'\\n' \r\n","    with open(file,'r') as ff:\r\n","        test = ff.readlines()\r\n","    test = [i.replace('\\n','') for i in test]\r\n","    test = [i.split(' ') for i in test]\r\n","    \r\n","    #reshape data\r\n","    text_final = []\r\n","    BIO_final = [] \r\n","    text_tmp = []\r\n","    BIO_tmp = []\r\n","    for i in test:\r\n","        if len(i) > 1:\r\n","            text_tmp.extend([i[0]])\r\n","            BIO_tmp.append(i[1])\r\n","        elif len(i) == 1:\r\n","            text_final.append(text_tmp)\r\n","            BIO_final.append(BIO_tmp)\r\n","            text_tmp = []\r\n","            BIO_tmp = []       \r\n","    cls_value = [i.pop(0) for i in BIO_final]\r\n","    [i.pop(0) for i in text_final]\r\n","    return text_final, BIO_final, cls_value\r\n","\r\n","def label2index(label,label_dict):\r\n","    index_label = []\r\n","    for i in label:\r\n","        lab_tmp = [label_dict[j] for j in i] \r\n","        index_label.append(lab_tmp)\r\n","    return index_label"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"p-eKLmSeIl1O"},"source":["# label = [['B-item'], ['B-item'], ['B-item']]\r\n","# label_dict = {'O':0 , 'B-item':1 , 'I-item':2 ,'B-money':3 ,'I-money':4}\r\n","# num_label = label2index(label,label_dict)\r\n","# print(num_label)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cLBl9xvYEfBX","executionInfo":{"status":"ok","timestamp":1610186032164,"user_tz":-480,"elapsed":1054,"user":{"displayName":"曹峻瑋","photoUrl":"","userId":"01275852677691706051"}}},"source":["def get_representation(output):\r\n","    # shape: (seq_len, vocab_size)\r\n","    hidden_states = output[1]\r\n","\r\n","    token_embeddings = torch.stack(hidden_states, dim=0)\r\n","    # remove dimension 1 (batches)\r\n","    token_embeddings = torch.squeeze(token_embeddings, dim=1)\r\n","    # swap dimension 0 and 1\r\n","    token_embeddings = token_embeddings.permute(1, 0, 2)\r\n","    # the last hidden layer output (2+seq_len, 768)\r\n","    hidden_states = [token[-1] for token in token_embeddings]\r\n","\r\n","    return hidden_states"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"AGdG6NfhPIHN","executionInfo":{"status":"ok","timestamp":1610186033640,"user_tz":-480,"elapsed":1190,"user":{"displayName":"曹峻瑋","photoUrl":"","userId":"01275852677691706051"}}},"source":["n_classes = 5\r\n","intent_to_id = {\r\n","    \"income\": 0,\r\n","    \"expense\": 1\r\n","}\r\n","slot_to_id = {\r\n","    \"O\": 0,\r\n","    \"B-item\": 1,\r\n","    \"I-item\": 2,\r\n","    \"B-money\": 3,\r\n","    \"I-money\": 4\r\n","}\r\n","\r\n","id_to_intent = {v: k for (k, v) in intent_to_id.items()}\r\n","id_to_slot = {v: k for (k, v) in slot_to_id.items()}"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"e7usB3efGYxF","executionInfo":{"status":"ok","timestamp":1610186034650,"user_tz":-480,"elapsed":871,"user":{"displayName":"曹峻瑋","photoUrl":"","userId":"01275852677691706051"}}},"source":["# decoder for ID\r\n","class ID_module(nn.Module):\r\n","    def __init__(self):\r\n","        super(ID_module, self).__init__()\r\n","        self.linear = nn.Linear(768, 2)\r\n","    \r\n","    def forward(self, X):\r\n","        X = self.linear(X)\r\n","        return X"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"7DGlvibnGcqK","executionInfo":{"status":"ok","timestamp":1610186035216,"user_tz":-480,"elapsed":855,"user":{"displayName":"曹峻瑋","photoUrl":"","userId":"01275852677691706051"}}},"source":["# decoder for SF\r\n","class SF_module(nn.Module):\r\n","    def __init__(self):\r\n","        super(SF_module, self).__init__()\r\n","        self.linear = nn.Linear(768, n_classes)\r\n","    \r\n","    def forward(self, X):\r\n","        X = self.linear(X)\r\n","        return X"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"EQY0EPsBXNte","executionInfo":{"status":"ok","timestamp":1610186036586,"user_tz":-480,"elapsed":722,"user":{"displayName":"曹峻瑋","photoUrl":"","userId":"01275852677691706051"}}},"source":["# dataset: [[sen_1, 1], [sen_2, 1], [sen_3, 2], ...]\r\n","\r\n","def train_ID(ID_model, dataset, epochs, lr):\r\n","    ID_model.train()\r\n","    \r\n","    criterion = nn.CrossEntropyLoss()\r\n","    optimizer = optim.SGD(ID_model.parameters(), lr=lr, momentum=0.9)\r\n","\r\n","    for epoch in range(epochs):\r\n","        totalLoss = 0\r\n","        accuracy = 0\r\n","        count = 0\r\n","\r\n","        for X, y in dataset:\r\n","            y = torch.LongTensor([int(y)]).to(DEVICE)\r\n","            optimizer.zero_grad()\r\n","\r\n","            X = \" \".join(X)\r\n","            X_encoding = tokenizer.encode_plus(X, add_special_tokens=True, return_tensors='pt')\r\n","            X_ids = X_encoding['input_ids'].to(DEVICE)\r\n","\r\n","            with torch.no_grad():\r\n","                output = model(X_ids)\r\n","                \r\n","            # get the 768-d representation of [CLS]\r\n","            ID_input = get_representation(output)[0]\r\n","            ID_output = ID_model(ID_input).unsqueeze(0)\r\n","\r\n","            ##### record the loss? #####\r\n","            _, predicted = torch.max(ID_output.data, 1)\r\n","            count += len(X_ids) - 2\r\n","            accuracy += (predicted == y).sum().item()\r\n","\r\n","            loss = criterion(ID_output, y)\r\n","\r\n","            totalLoss += loss.item()\r\n","            # print(\"Loss: {}\".format(totalLoss))\r\n","            \r\n","            loss.backward()\r\n","            optimizer.step()\r\n","\r\n","        print(\"Train Loss: {}\".format(totalLoss / count))\r\n","        print(\"Train Accuracy: {}\".format(accuracy / count))\r\n","\r\n","    return ID_model"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"Tcf595K5E0ZW","executionInfo":{"status":"ok","timestamp":1610186038838,"user_tz":-480,"elapsed":1025,"user":{"displayName":"曹峻瑋","photoUrl":"","userId":"01275852677691706051"}}},"source":["# dataset: [[sen_1, [1,1,1,2,3,...]], [sen_2, [1,1,2,3,...]], [sen_3, [1,1,1,2,...]], ...]\r\n","\r\n","def train_SF(SF_model, dataset, epochs, lr):\r\n","    SF_model.train()\r\n","\r\n","    criterion = nn.CrossEntropyLoss()\r\n","    optimizer = optim.SGD(SF_model.parameters(), lr=lr, momentum=0.9)\r\n","\r\n","    for epoch in range(epochs):\r\n","        totalLoss = 0\r\n","        count = 0\r\n","        y_true = []\r\n","        y_pred = []\r\n","\r\n","        for X, y in dataset:\r\n","            y_true.extend(y)\r\n","            optimizer.zero_grad()\r\n","\r\n","            X = \" \".join(X)\r\n","            X_encoding = tokenizer.encode_plus(X, add_special_tokens=True, return_tensors='pt')\r\n","            X_ids = X_encoding['input_ids'].to(DEVICE)\r\n","\r\n","            with torch.no_grad():\r\n","                output = model(X_ids)\r\n","\r\n","            # get the 768-d representation of other tokens than [CLS]\r\n","            SF_input = get_representation(output)[1:-1]\r\n","            # for each position, predict a label with SF_model and back propagate the loss\r\n","            for p in range(len(X_ids[0]) - 2):\r\n","                y_single = torch.LongTensor([y[p]]).to(DEVICE)\r\n","                SF_output = SF_model(SF_input[p]).unsqueeze(0)\r\n","\r\n","                # print(y_single)\r\n","\r\n","                y_pred.append(torch.argmax(SF_output).item())\r\n","\r\n","                # record the loss\r\n","                loss = criterion(SF_output, y_single)\r\n","                # print(\"Loss: {:2f}\".format(loss))\r\n","                count += 1\r\n","                totalLoss += loss.item()*5\r\n","    \r\n","                loss.backward()\r\n","                optimizer.step()\r\n","\r\n","        print(\"Train Loss: {:2f}\".format(totalLoss / count))\r\n","        # print(\"Train Loss: {}\".format(totalLoss))\r\n","        # print f1-score\r\n","        print( \"F1 score: {:2f}\".format(f1_score(y_true, y_pred, average = 'macro')) )\r\n","\r\n","    return SF_model"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lFgvJqqP5_3S"},"source":["## Validate"]},{"cell_type":"code","metadata":{"id":"HEa0ozSJZMCn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WuDkwR2G50wq"},"source":["## Predict"]},{"cell_type":"code","metadata":{"id":"puT6n5JWWvQu","executionInfo":{"status":"ok","timestamp":1610186046216,"user_tz":-480,"elapsed":1316,"user":{"displayName":"曹峻瑋","photoUrl":"","userId":"01275852677691706051"}}},"source":["# sentence: [\"A\", \"B\", \"C\", ...]\r\n","def predict_ID(ID_model, sentence):\r\n","    X = \" \".join(sentence)\r\n","    X_encoding = tokenizer.encode_plus(X, add_special_tokens=True, return_tensors='pt')\r\n","    X_ids = X_encoding['input_ids'].to(DEVICE)\r\n","\r\n","    with torch.no_grad():\r\n","        output = model(X_ids)\r\n","    \r\n","    # print(output[1][0].shape)\r\n","\r\n","    ID_model.eval()\r\n","    ID_input = get_representation(output)[0]\r\n","    ID_output = ID_model(ID_input)\r\n","    result = torch.argmax(ID_output).item()\r\n","\r\n","    return result"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"kjIiioM0OpI3","executionInfo":{"status":"ok","timestamp":1610186048567,"user_tz":-480,"elapsed":980,"user":{"displayName":"曹峻瑋","photoUrl":"","userId":"01275852677691706051"}}},"source":["# sentence: [\"A\", \"B\", \"C\", ...]\r\n","def predict_SF(SF_model, sentence):\r\n","    X = \" \".join(sentence)\r\n","    X_encoding = tokenizer.encode_plus(X, add_special_tokens=True, return_tensors='pt')\r\n","    X_ids = X_encoding['input_ids'].to(DEVICE)\r\n","\r\n","    y_pred = []\r\n","\r\n","    with torch.no_grad():\r\n","        output = model(X_ids)\r\n","    \r\n","    SF_model.eval()\r\n","    SF_input = get_representation(output)[1:-1]\r\n","    for p in range(len(X_ids[0]) - 2):\r\n","        SF_output = SF_model(SF_input[p]).unsqueeze(0)\r\n","        y_pred.append(torch.argmax(SF_output).item())\r\n","\r\n","    return y_pred"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wealBifT5rNu"},"source":["## Data"]},{"cell_type":"code","metadata":{"id":"j_dbhUI_Ydfw"},"source":["data_file = \"/content/drive/MyDrive/NLP/Final Project/all_data_income99_expense_99.txt\"\r\n","text_final, BIO_final, cls_value = read_data(data_file)\r\n","\r\n","# for sentence in text_final:\r\n","#     ID_pred = predict_ID(ID_classifier, sentence)\r\n","#     print(\"\".join(sentence))\r\n","#     print(\"ID =\", ID_pred.item())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WhU_yiuRamZv","executionInfo":{"status":"ok","timestamp":1610167721760,"user_tz":-480,"elapsed":921,"user":{"displayName":"曹峻瑋","photoUrl":"","userId":"01275852677691706051"}}},"source":["random.seed(42)\r\n","ID_dataset = [z for z in zip(text_final, cls_value)]\r\n","random.shuffle(ID_dataset)\r\n","random.seed(42)\r\n","\r\n","BIO_final_id = label2index(BIO_final, label_dict)\r\n","SF_dataset = [z for z in zip(text_final, BIO_final_id)]\r\n","random.shuffle(SF_dataset)"],"execution_count":91,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9eGjF7Rs5vv0"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"1fdWvtcpF6J_","executionInfo":{"status":"ok","timestamp":1610186052765,"user_tz":-480,"elapsed":943,"user":{"displayName":"曹峻瑋","photoUrl":"","userId":"01275852677691706051"}}},"source":["ID_classifier = ID_module().to(DEVICE)\r\n","SF_classifier = SF_module().to(DEVICE)"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"YdoVqun7b79L"},"source":["ID_model_path = \"/content/drive/MyDrive/NLP/Final Project/ID_model\"\r\n","# load model\r\n","ID_classifier = ID_module().to(DEVICE)\r\n","ID_classifier.load_state_dict(torch.load(ID_model_path))\r\n","\r\n","# train model\r\n","# ID_classifier = train_ID(ID_classifier, epochs=10, lr=1e-3, dataset=ID_dataset)\r\n","\r\n","# save model\r\n","# torch.save(ID_classifier.state_dict(), ID_model_path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5Mll400ji5KH","executionInfo":{"status":"ok","timestamp":1610169935193,"user_tz":-480,"elapsed":890,"user":{"displayName":"曹峻瑋","photoUrl":"","userId":"01275852677691706051"}},"outputId":"761a027e-46cb-4475-f164-d6701cd19815"},"source":["# sentences = [\"今天買了甜不辣花了20塊\",\"昨天做空期貨賺了120000\"]\r\n","# for sen in sentences:\r\n","#     answer = predict_ID(ID_model=ID_classifier, sentence=sen)\r\n","#     print(answer)"],"execution_count":116,"outputs":[{"output_type":"stream","text":["1\n","0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mYuHHmduSaFe"},"source":["SF_model_path = \"/content/drive/MyDrive/NLP/Final Project/SF_model\"\r\n","# load model\r\n","SF_model = SF_module().to(DEVICE)\r\n","SF_model.load_state_dict(torch.load(SF_model_path))\r\n","\r\n","# train model\r\n","# SF_model = train_SF(SF_classifier, epochs=5, lr=1e-3, dataset=SF_dataset)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J-MvBw-cXnf5","executionInfo":{"status":"ok","timestamp":1610169978104,"user_tz":-480,"elapsed":881,"user":{"displayName":"曹峻瑋","photoUrl":"","userId":"01275852677691706051"}}},"source":["# save model\r\n","# torch.save(SF_model.state_dict(), SF_model_path)"],"execution_count":117,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vWcqvAG8PBOV","executionInfo":{"status":"ok","timestamp":1610186126396,"user_tz":-480,"elapsed":1489,"user":{"displayName":"曹峻瑋","photoUrl":"","userId":"01275852677691706051"}},"outputId":"47ed5d58-d77f-4e39-cbbb-f7d8ad118e97"},"source":["sentences = [\"摔破花瓶賠了2000塊\"]\r\n","for sen in sentences:\r\n","    intent = predict_ID(ID_model=ID_classifier, sentence = sen)\r\n","    slots = predict_SF(SF_model=SF_model, sentence = sen)\r\n","    print(sen)\r\n","    print(id_to_intent[intent])\r\n","    print([id_to_slot[s] for s in slots])\r\n","    print('\\n')"],"execution_count":19,"outputs":[{"output_type":"stream","text":["摔破花瓶賠了2000塊\n","income\n","['O', 'O', 'O', 'I-item', 'O', 'O', 'B-money', 'I-money', 'I-money', 'I-money', 'I-money']\n","\n","\n"],"name":"stdout"}]}]}